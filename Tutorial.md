# Tutorial of single-cell RNA-seq data analysis in R
#### Created by Zhisong He (2020-03-19)
### Table of Content
  * [Introduction](#introduction)
  * [Preparation](#preparation)
  * [Now let's start](#now-lets-start)
    * [Step 0. Import Seurat package](#step-0-import-seurat-package)
	* [Step 1. Create a Seurat object](#step-1-create-a-seurat-object)
	* [Step 2. Quality control](#step-2-quality-control)
	* [Step 3. Normalization](#step-3-normalization)
	* [Step 4. Feature selection for following heterogeneity analysis](#step-4-feature-selection-for-following-heterogeneity-analysis)
	* [Step 5. Data scaling](#step-5-data-scaling)
	* [(Optional and advanced) Alternative step 3-5: to use SCTransform](#optional-and-advanced-alternative-step-3-5-to-use-sctransform)
	* [Step 6. Linear dimension reduction using principal component analysis (PCA)](#step-6-linear-dimension-reduction-using-principal-component-analysis-pca)
	* [Step 7. Non-linear dimension reduction for visualization](#step-7-non-linear-dimension-reduction-for-visualization)
	* [Step 8. Cluster the cells](#step-8-cluster-the-cells)
	* [Step 9. Annotate cell clusters](#step-9-annotate-cell-clusters)

## Introduction
After getting your scRNA-seq data of your samples, analyzing them properly is the next crucial step.

Nowadays, there are multiple toolkits and analytic framework developed to provide facilitate scRNA-seq data analysis. These options include but not limit to [Seurat](https://satijalab.org/seurat/), developped by Rahul Satija's Lab, in R, and [scanpy](https://icb-scanpy.readthedocs-hosted.com/en/stable/), developped by Fabian Theis's Lab, in Python. Both toolkits provide functions and rich parameter sets to serve most of the routine analysis that one usually does to get scRNA-seq data analysis done. On the other hand, one should aware that these analytic frameworks do not cover all the interesting analysis that one do when analyzing data. It is also important to get to know other tools for scRNA-seq data analysis.

But as here it is a tutorial to the starters, we will mostly introduce how to use Seurat to analyze your scRNA-seq data in R. At the end, we will also mention some other additional tools (e.g. presto, destiny, Harmony, simspec, etc.) which provide additional functionalities that you may miss if you only use Seurat.

## Preparation
This tutorial assumes that the sequencing data preprocessing steps, including base calling, mapping and read counting, have been done. 10x Genomics has its own analysis pipeline [Cell Ranger](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger) for them, if the experiment is done with 10x Genomics Chromium Single Cell Gene Expression Solution. At the end of the Cell Ranger pipeline, a counting matrix is generated. However, if the data is generated by other scRNA-seq experiments (e.g. well-based experiments with the Smart-Seq2 protocol) rather than 10x Genomics technology, the Cell Ranger pipeline is likely inapplicable, and one will have to find their own solutions to generate the counting matrix.

With this tutorial, there are two data sets (DS1 and DS2), both generated by 10x Genomics and preprocessed by Cell Ranger. They are both public scRNA-seq data of human cerebral organoids and are parts of the data presented in this [paper](https://www.nature.com/articles/s41586-019-1654-9). The first part of this tutorial, which includes most of the general analysis, is based on DS1, while the second part, which focuses on data integration and batch effect correction, is based on both data sets.

## Now let's start
### Step 0. Import Seurat package
First of all, please make sure that Seurat is installed in your R.
```R
library(Seurat)
```
This import your installed Seurat package into your current R session. No error should be seen but some verbose information is likely. If it warns you that the package is unavailable, please install Seurat first
```R
install.packages("Seurat")
library(Seurat)
```
### Step 1. Create a Seurat object
Seurat implements a new data type which is named by its own 'Seurat', which allows Seurat to store all the steps and results along the whole analysis. Therefore, the first step is to read in the data and create a Seurat object. Seurat has an easy solution for data generated by 10x experiment.
```R
counts <- Read10X(data.dir = "data/DS1/")
seurat <- CreateSeuratObject(counts, project="DS1")
```
When is done by the ```Read10X``` function is to read in the matrix and rename its row names and col names by gene symbols and cell barcodes. Alternatively, one can do this manually, while would be probably what one would have to do when the data is not generated by 10x
```R
library(Matrix)
counts <- readMM("data/DS1/matrix.mtx.gz")
barcodes <- read.table("data/DS1/barcodes.tsv.gz", stringsAsFactors=F)[,1]
features <- read.csv("data/DS1/features.tsv.gz", stringsAsFactors=F, sep="\t", header=F)
rownames(counts) <- make.names(features[,2], unique=T)
colnames(counts) <- barcodes

seurat <- CreateSeuratObject(counts, project="DS1")
```
If you look at the [Seurat tutorial](https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html), you would notice that some extra options are added to the ```CreateSeuratObj``` function, such as ```min.cells``` and ```min.features```. Setting these two parameters calls an initial filtering to the data, removing all genes with reads detected in too few cells, as well as cells with too few genes detected, from the very beginning. This is also fine, but I personally recommend to keep all genes (i.e. default or ```min.cells = 0```)

### Step 2. Quality control
After creating the Seurat object, the next step is to do quality control on the data. The most common quality control is to filter out
1. Cells with too few genes detected. They usually represent cells which are not sequenced deep enough for reliable characterization
2. Cells with too many genes detected. They may represent doublets or multiplets (i.e. two or more cells in the same droplet, therefore share the same cell barcode)
3. Cells with high mitochondrial transcript percentage. As most of the scRNA-seq experiments use oligo-T to capture mRNAs, mitochondrial transcripts should be relatively under-representative due to their lack of poly-A tails, but it is avoidless that some mitochondrial transcripts are captured. Meanwhile, there are also evidences that stable poly-A tails exist in some mitochondrial transcripts but serve as a marker for degradation (e.g. this [paper](https://mcb.asm.org/content/25/15/6427.long)). Together, cells with high mitochondrial transcript percentage may represent cells under stress like hypoxia that produce lots of mitochondria, or produce abnormally high amount of truncated mitochondrial transcripts.

While numbers of detected genes are summarized by Seurat automatically when creating the Seurat object (with default feature name nFeature_RNA; nCount_RNA is the number of detected transcripts), one needs to calculate mitochondial transcript percentages manually. Still, Seurat has a way to make it easier
```R
seurat[["percent.mt"]] <- PercentageFeatureSet(seurat, pattern = "^MT[-\\.]")
```

Please note that there is no one-for-all filtering criteria, as the normal ranges of these metrics can vary dramatically from one experiment to another, depending on sample origins as well as reagants and sequencing depths. One suggestion here is to **ONLY FILTER OUT OUTLIER CELLS**, i.e. those **minority** of cells with certain QC metrics clearly exceeding or below other cells. To do that, one need to first know how the distribution of these values in the data. Make a violin plot for each of the metrics is what one can do.
```R
VlnPlot(seurat, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)
```
<img src="images/vlnplot_QC.png" align="centre" /><br/><br/>
Or if you don't like the dots
```R
VlnPlot(seurat, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3, pt.size=0)
```
<img src="images/vlnplot_QC_nopt.png" align="centre" /><br/><br/>

And as one would expect, the numbers of detected genes and numbers of detected transcripts are well correlated across cells while mitochondrial transcript percentage is not.
```R
library(patchwork)
plot1 <- FeatureScatter(seurat, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(seurat, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot1 + plot2
```
<span style="font-size:0.8em">*P.S. patchwork is an R package developped to facilitate layout of plots produced by ggplot2 (Seurat uses ggplot2 to produce plots if you use the plotting functions in the Seurat package). Without patchwork, it is illegal to run ```plot1 + plot2```.*</span>
<img src="images/scatterplot_QCmetrics.png" align="centre" /><br/><br/>

Therefore, we only need to set cutoffs to either the detected gene number or detected transcript number, combining with an upper threshold of mitochondrial transcript percentage, for the QC. For instance, for this data set, detected gene numbers between 500 and 5000, and mitochondrial transcript percentage lower than 5% would be quite reasonable, but it is completely fine to use different thresholds.
```R
seurat <- subset(seurat, subset = nFeature_RNA > 500 & nFeature_RNA < 5000 & percent.mt < 5)
```

### Step 3. Normalization
Similar to bulk RNA-seq, the amount of captured RNA in each cell is different, therefore a direct comparison of the captured transcript number of each gene across different cells is definitely not the right way to do. A normalization step, aiming to make gene expression levels of different cells comparable, is therefore necessary. The most commonly used normalization in scRNA-seq data analysis is very similar to the concept of TPM (Transcripts Per Million reads), to normalizes the feature expression measurements for each cell by the total expression, and then multiplies this by a scale factor (10000 by default). At the end, the resulted expression levels are log-transformed so that the resulted values can better fit normal distribution. It is worth to mention, that before doing log-transform, one pseudocount is added to every value, so that genes with zero transcript detected in a cell still present present values of zero after log-transform.
```R
seurat <- NormalizeData(seurat)
```
In principle there are several parameters one can set in the ```NormalizeData``` function but in most of the time one just needs the default setting.

### Step 4. Feature selection for following heterogeneity analysis
The biggest advantage of doing scRNA-seq experiment is the potential to look into cell heterogeneity of samples, by looking for cell groups with distinct molecular signatures. However, not every gene has the same level of information and the same contribution when trying to identify different cell groups. For instance, genes with low expression levels, and those with similar expression levels across all cells, are not very informative and may introduce troubles by diluting differences among different cell groups. Therefore, doing a proper feature selection is very necessary when analyzing scRNA-seq data.

In Seurat, or more general in scRNA-seq data analysis, this step usually refers to identification of highly variable features/genes, which means to identify genes with the most varied expression levels in cells.
```R
seurat <- FindVariableFeatures(seurat, nfeatures = 3000)
```
By default, Seurat calculates standardized variance of each gene, and pick the top 2000 ones as the highly variable features. One can change the number of highly variable features easily by giving the ```nfeatures``` option (here the top 3000 genes are used).

There is no good criteria to determine how many highly variable features to use. Sometimes one needs to do some iteration to pick the number giving the most clear and interpretable result. In most of time, a value between 2000 to 5000 is OK and using a different value usually don't affect the results very much.

One can visualize the result via a variable feature plot but this is very optional.
```R
top_features <- head(VariableFeatures(seurat), 20)
plot1 <- VariableFeaturePlot(seurat)
plot2 <- LabelPoints(plot = plot1, points = top_features, repel = TRUE)
plot1 + plot2
```
<img src="images/variablefeatures.png" align="centre" /><br/><br/>

### Step 5. Data scaling
Since different genes have different base expression levels and variations, their contributions to the analysis are different if no data transformation is done. This is not something we want as we don't want our analysis only represent the behaviors of highly expressed genes. Therefore, as what is usually done in any data science field, a scaling to the data is applied to the selected features.
```R
seurat <- ScaleData(seurat)
```

At this step, one can also remove unwanted source of variation from the data set by setting the parameter ```var.to.regress```. For instance,
```R
seurat <- ScaleData(seurat, vars.to.regress = c("nFeature_RNA", "percent.mt"))
```
Variables which are commonly considered to regress out include the number of detected genes/transcripts (nFeature_RNA / nCount_RNA), mitochondrial transcript percentage (percent.mt), and cell cycle related varaibles (will mention later). However, doing so dramatically slow down the whole process and the result is not necessarily improved. Therefore, a common suggestion is to first of all not to do any regress-out, check the result, and if the unwanted variation source dominated the heterogeneity, try to regress out the variable and see whether the result improves.

### (Optional and advanced) Alternative step 3-5: to use SCTransform
One problem of doing the typical log-normalization is [introducing the zero-inflation artifact](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) to the scRNA-seq data. To better resolve this issue, Hafemeister and Satija introduced an R package ```sctransform```, which uses a regularized negative binomial regression model to normalize scRNA-seq data. Seurat has a wrapper function ```SCTransform```.
```R
seurat <- SCTransform(seurat, variable.features.n = 3000)
```
The ```variable.features.n``` controls the number of highly variable features to identify. One can also put in unwanted variation source to the model to try to remove them. For instance,
```R
seurat <- SCTransform(seurat, vars.to.regress = c("nFeature_RNA", "percent.mt"), variable.features.n = 3000)
```
This operation combines normalization, scaling and highly variable feature identification so it essentially replaces the above Step 3 to step 5. Drawbacks of running ```SCTransform``` include
1. It is slow.
2. It makes the normalized expression measurements data-dependent. In the typical procedure, the normalization only relies on the cell itself; in ```SCTransform```, however, information from the other cells in the same data set is involved during normalization. This potentially introduces problems when multiple data sets are needed to be compared, as technically speaking, the normalized expression measurements of two data sets normalized using ```SCTransform``` on their own are not comparable.
3. There are steps in ```SCTransform``` which involve random sampling to speed up the computation. It means there is stochastics in ```SCTransform``` so the result is different from one time to another, even if it is applied to the same data set.

Therefore, use it wisely.

### Step 6. Linear dimension reduction using principal component analysis (PCA)
In principle one can start to look at cell heterogeneity after identification of highly variable genes and scaling the data. However, applying a linear dimension reduction before doing any further analysis is strongly recommended and sometimes even seen as compulsory. The benefit of doing such a dimension reduction includes but not limits to:
1. The data becomes much more compact so that computation becomes much faster.
2. As scRNA-seq data is intrincically sparse, summarizing measurements of related features greatly enhances the signal robustness

Drawback? Basically nothing. Well, one needs some extra lines in the script and needs to decide the number of reduced dimension to use in the following analysis, but that's it. If so, what's the point not to do it?

For scRNA-seq data, the linear dimension reduction mostly refers to principal component analysis, or PCA.
```R
seurat <- RunPCA(seurat, npcs = 50)
```
In principle, the number of PCs in your data that one can calculate is the smaller value between the number of highly variable genes and the number of cells. However, most of these PCs are not informative and only represent random noise. Only the top PCs are informative and represent differences among cell groups. Therefore, instead of calculating all possible PCs, Seurat uses truncated PCA to only calculate the first PCs, by default the top 50 PCs. One can change that by setting the ```npcs``` parameter.

Even then, one doesn't necessarily use all the calculated PCs. Indeed, determinting how many top PCs to use is an art. There is no golden standard, and everyone has his/her own understanding. Usually, people use the elbowplot to assist making the decision.
```R
ElbowPlot(seurat, ndims = ncol(Embeddings(seurat, "pca")))
```
<span style="font-size:0.8em">*P.S. ```Embeddings``` is the function in Seurat to obtain the dimension reduction result given the name of the dimension reduction of interest. By default, the ```RunPCA``` function stores the PCA result in the embedding called 'pca', with each column being one PC. So here it tells Seurat to make the elbowplots to show the standarized variation of all the PCs that are calculated*</span>

<img src="images/elbowplot.png" align="centre" /><br/><br/>

As it's defined, low-rank PCs have smaller standard deviations. However, such decrease of standard deviation is not linear. It drops dramatically at the very beginning, and then slow down and soon becomes pretty flat. One would then assume that the first phase of the curve represent the 'real' signal related to cell group differences, while the second phase represent mostly fluctuation of measurement or the stochastic nature of individual cells. To that perspective, choosing the top-15 PCs is probably good and PCs ranked lower than 20 look quite unnecessary. However, even though this is a pretty good reference, it is far from perfect:
  * It is very difficult to precisely define the elbow point or turning point of the curve, as it is usually not a perfect elbow.
  * Higher-ranked PCs do explain more variation than lower-ranked PCs, but in real world more explained variations don't necessaily mean more informative. Sometimes there are interesting but weak signals buried among noises and can only recovered in some lower-ranked PCs.

There is another procedure implemented in Seurat called ```JackStraw``` which can also serve as another reference. However, to our experience, it is very slow because it relies on data permutations and essentially it does not provide much more information than the elbow plot. It does estimates statistical significance of each PC but similarly, a 'significant' PC doesn't mean it is informative. And when cell number increases, more and more PCs become statistically 'significant' even though their explained variation is not substantial. People interested in this method can take a look at the Seurat [vignette](https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html).

Besides making the decision unbiasly, one can also check for each of the top PCs which genes are mostly contributing. This can be informative if one knows these genes and the biology behind. This gives opportunities to understand or guess the biological implication of each of the top PCs, so that one can pick those representing useful information. 
```R
PCHeatmap(seurat, dims = 1:20, cells = 500, balanced = TRUE, ncol = 4)
```
<img src="images/pcheatmap.png" align="centre" /><br/><br/>
Please be aware that it is not recommended to choose ONLY PCs represented by the "interesting" genes. There is a huge chance one misses interesting but unexpected phenomena by doing that.

In this example, we would use the top-20 PCs for the following analysis. Again, it is absolutely fine to use more the fewer PCs, and in practice this sometimes needs some iteration to make the final decision. Meanwhile, for most of the data, a PC number ranging from 10 to 50 would be reasonable and in many cases it won't affect the conclusion very much.

### Step 7. Non-linear dimension reduction for visualization
A linear dimension reduction has both pros and cons. The good side is that every PC is a linear combination of gene expression so interpretation of PCs are straightforward. Also the data is compressed but not disorted, therefore information in the data is largely remained. The bad side, on the other hand, is that one usually needs more than 10 PCs to cover most of the information. This is fine for most of the analysis, but not for visualization where it is impossible to go over three dimensions for ordinary persons.

To overcome this issue, non-linear dimension reductions is introduced. The most commonly used non-linear dimension reduction methods in scRNA-seq data analysis are t-distributed Stochastic Neighbor Embedding (t-SNE) and Uniform Manifold Approximation and Projection (UMAP). Both methods try to place every sample in a low-dimensional space (2D/3D), so that distances or neighborhood relationships between different samples (here cells) in the original space are largely retained in the low-dimensional space. The detailed mathematically descriptions of the two methods are out of the scope of this tutorial, but for those who are interested in, you may check this [video](https://www.youtube.com/watch?v=RJVL80Gg3lA&list=UUtXKDgv1AVoG88PLl8nGXmw) for tSNE, and this [blog](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668) of Nikolay Oskolkov for UMAP. There are also more methods to create other low-dimensional embeddings for visualization, including but not limiting to [SPRING](https://kleintools.hms.harvard.edu/tools/spring.html), [PHATE](https://phate.readthedocs.io/en/stable/). Now let's focus on tSNE and UMAP which Seurat has included. The top PCs in the PCA analysis are used as the input to create a tSNE and UMAP embedding of the data.
```R
seurat <- RunTSNE(seurat, dims = 1:20)
seurat <- RunUMAP(seurat, dims = 1:20)
```
<span style="font-size:0.8em">*P.S. technically one can directly use the scaled expression of highly variable genes for these. It is however not recommended, as it is much slower and probably more noisy.*</span>

The results can be then visualized:
```R
plot1 <- TSNEPlot(seurat)
plot2 <- UMAPPlot(seurat)
plot1 + plot2
```
<img src="images/tsne_umap_nogroup.png" align="centre" /><br/><br/>
It is a hot topic whether tSNE or UMAP is superior to the other (for instance, this [blog](https://towardsdatascience.com/tsne-vs-umap-global-structure-4d8045acba17) of Nikolay Oskolkov and this [paper](https://www.biorxiv.org/content/10.1101/2019.12.19.877522v1) by Kobak and Linderman). To our experience, they both have their pros and cons and neither always work better than the other one. TSNE provides great visualization when cells form distinct cell groups, while UMAP perserves trajectory-like structure better when data contains 'continuum', e.g. the continuous cell state change during development and differentiation. It is therefore good to try both, and choose the one works better for your data.

Once the tSNE or UMAP embedding is created, one can start to check whether certain cell types or cell states exist in the data, by doing feature plots of some known canonical markers of the cell types of interest.
```R
plot1 <- FeaturePlot(seurat, c("MKI67","NES","DCX","FOXG1","DLX2","EMX1","OTX2","LHX9","TFAP2A"), ncol=3, reduction = "tsne")
plot2 <- FeaturePlot(seurat, c("MKI67","NES","DCX","FOXG1","DLX2","EMX1","OTX2","LHX9","TFAP2A"), ncol=3, reduction = "umap")
plot1 / plot2
```
<img src="images/tsne_umap_featureplots.png" align="centre" /><br/><br/>
For people who are not familiar with those genes:
  * MKI67: a marker of G2M phase of cell cycle
  * NES: a neural progenitor marker
  * DCX: a pan-neuron marker
  * FOXG1: a telencephalon marker
  * DLX2: a ventral telencephalon marker
  * EMX1: a dorsal telencephalon (cortex) marker
  * OTX2: a diencephalon and midbrain inhibitory neuron marker
  * LHX9: a diencephalon and midbrain excitatory neuron marker
  * TFAP2A: a midbrain-hindbrain boundary and hindbrain marker

So now we have some idea about what kinds of cells exist in this data.

### Step 8. Cluster the cells
Doing feature plot of markers is usually a good way to start with when exploring scRNA-seq data. However, to more comprehensively understand the underlying heterogeneity in the data, it is necessary to identify cell groups with an unbiased manner. This is what clustering does. In principle, one can apply any clustering methods, including those widely used in bulk RNA-seq data analysis such as hierarchical clustering and k-means, to the scRNA-seq data. However, in practice, this is very difficult, as the sample size in scRNA-seq data is too much larger (one 10x experiment usually gives several thousands of cells). It would be extremely slow to use these methods. In addition, due to the intrinsic sparseness of scRNA-seq data, even if data is denoised by dimension reduction like PCA, differences between different cells are not as well quantitative as those of bulk RNA-seq data. Therefore, the more commonly used clustering methods in scRNA-seq data analysis is graph-based community identification algorithm. Here, graph is the mathematical concept, where there is a set of objects, and some pairs of these objects are related with each other; or in a simplified way, a network of something, and here, a network of cells.

First of all, a k-nearest neighbor network of cells is generated. Every cells is firstly connected to cells with the shortest distances, based on their corresponding PC values. Only cell pairs which are neighbors of each other are considered as connected. Proportion of shared neighbors between every cell pairs is then calculated and used to describe the strength of the connection between two cells. Weak connections are trimmed. This gives the resulted Shared Nearest Neighbor (SNN) network. In practice, this is very simple in Seurat.
```R
seurat <- FindNeighbors(seurat, dims = 1:20)
```

With the network constructed, the louvain community identification algorithm is applied to the netowkr to look for communities in the network, i.e. cell groups that cells in the same group tend to connect with each other, while connections between cells in different groups are sparse.
```R
seurat <- FindClusters(seurat, resolution = 1)
```
Here, the ```resolution``` parameter is used to control whether the major and coarsed cell groups (e.g. major cell types), or the smaller but finer cell groups are returned (e.g. cell subtypes). The commonly used resolution ranges between 0.1 and 1, and which is the best option largely depends on the aim of the analysis. Here, a high resolution parameter is used to get a finer clustering. One can run multiple times of the ```FindClusters``` function with different resolutions. The newest clustering result can be obtained by ```Idents(seurat)``` or ```seurat@active.ident```. Other clustering results are also stored, as different columns in the meta.data slot (```seurat@meta.data```)

Next is to visualize the clustering result using the tSNE and UMAP embeddings that are generated before.
```R
plot1 <- DimPlot(seurat, reduction = "tsne", label = TRUE)
plot2 <- DimPlot(seurat, reduction = "umap", label = TRUE)
plot1 + plot2
```
<span style="font-size:0.8em">*P.S. if you don't want to see the cluster labels, set ```label = FALSE``` or remove it (by default ```label``` is set to FALSE).*</span>

<img src="images/tsne_umap_cluster.png" align="centre" /><br/><br/>

### Step 9. Annotate cell clusters

