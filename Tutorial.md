# Tutorial of single-cell RNA-seq data analysis in R
#### Created by Zhisong He (2020-03-19)
### Table of Content
  * [Introduction](#introduction)
  * [Preparation](#preparation)
  * [Now let's start](#now-lets-start)
    * [Step 0. Import Seurat package](#step-0-import-seurat-package)
	* [Step 1. Create a Seurat object](#step-1-create-a-seurat-object)
	* [Step 2. Quality control](#step-2-quality-control)
	* [Step 3. Normalization](#step-3-normalization)
	* [Step 4. Feature selection for following heterogeneity analysis](#step-4-feature-selection-for-following-heterogeneity-analysis)
	* [Step 5. Data scaling](#step-5-data-scaling)
	* [(Optional and advanced) Alternative step 3-5: to use SCTransform](#optional-and-advanced-alternative-step-3-5-to-use-sctransform)
	* [Step 6. Linear dimension reduction using principal component analysis (PCA)](#step-6-linear-dimension-reduction-using-principal-component-analysis-pca)

## Introduction
After getting your scRNA-seq data of your samples, analyzing them properly is the next crucial step.

Nowadays, there are multiple toolkits and analytic framework developed to provide facilitate scRNA-seq data analysis. These options include but not limit to [Seurat](https://satijalab.org/seurat/), developped by Rahul Satija's Lab, in R, and [scanpy](https://icb-scanpy.readthedocs-hosted.com/en/stable/), developped by Fabian Theis's Lab, in Python. Both toolkits provide functions and rich parameter sets to serve most of the routine analysis that one usually does to get scRNA-seq data analysis done. On the other hand, one should aware that these analytic frameworks do not cover all the interesting analysis that one do when analyzing data. It is also important to get to know other tools for scRNA-seq data analysis.

But as here it is a tutorial to the starters, we will mostly introduce how to use Seurat to analyze your scRNA-seq data in R. At the end, we will also mention some other additional tools (e.g. presto, destiny, Harmony, simspec, etc.) which provide additional functionalities that you may miss if you only use Seurat.

## Preparation
This tutorial assumes that the sequencing data preprocessing steps, including base calling, mapping and read counting, have been done. 10x Genomics has its own analysis pipeline [Cell Ranger](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger) for them, if the experiment is done with 10x Genomics Chromium Single Cell Gene Expression Solution. At the end of the Cell Ranger pipeline, a counting matrix is generated. However, if the data is generated by other scRNA-seq experiments (e.g. well-based experiments with the Smart-Seq2 protocol) rather than 10x Genomics technology, the Cell Ranger pipeline is likely inapplicable, and one will have to find their own solutions to generate the counting matrix.

With this tutorial, there are two data sets (DS1 and DS2), both generated by 10x Genomics and preprocessed by Cell Ranger. They are both public scRNA-seq data of human cerebral organoids and are parts of the data presented in this [paper](https://www.nature.com/articles/s41586-019-1654-9). The first part of this tutorial, which includes most of the general analysis, is based on DS1, while the second part, which focuses on data integration and batch effect correction, is based on both data sets.

## Now let's start
### Step 0. Import Seurat package
First of all, please make sure that Seurat is installed in your R.
```R
library(Seurat)
```
This import your installed Seurat package into your current R session. No error should be seen but some verbose information is likely. If it warns you that the package is unavailable, please install Seurat first
```R
install.packages("Seurat")
library(Seurat)
```
### Step 1. Create a Seurat object
Seurat implements a new data type which is named by its own 'Seurat', which allows Seurat to store all the steps and results along the whole analysis. Therefore, the first step is to read in the data and create a Seurat object. Seurat has an easy solution for data generated by 10x experiment.
```R
counts <- Read10X(data.dir = "data/DS1/")
seurat <- CreateSeuratObject(counts, project="DS1")
```
When is done by the ```Read10X``` function is to read in the matrix and rename its row names and col names by gene symbols and cell barcodes. Alternatively, one can do this manually, while would be probably what one would have to do when the data is not generated by 10x
```R
library(Matrix)
counts <- readMM("data/DS1/matrix.mtx.gz")
barcodes <- read.table("data/DS1/barcodes.tsv.gz", stringsAsFactors=F)[,1]
features <- read.csv("data/DS1/features.tsv.gz", stringsAsFactors=F, sep="\t", header=F)
rownames(counts) <- make.names(features[,2], unique=T)
colnames(counts) <- barcodes

seurat <- CreateSeuratObject(counts, project="DS1")
```
If you look at the [Seurat tutorial](https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html), you would notice that some extra options are added to the ```CreateSeuratObj``` function, such as ```min.cells``` and ```min.features```. Setting these two parameters calls an initial filtering to the data, removing all genes with reads detected in too few cells, as well as cells with too few genes detected, from the very beginning. This is also fine, but I personally recommend to keep all genes (i.e. default or ```min.cells = 0```)

### Step 2. Quality control
After creating the Seurat object, the next step is to do quality control on the data. The most common quality control is to filter out
1. Cells with too few genes detected. They usually represent cells which are not sequenced deep enough for reliable characterization
2. Cells with too many genes detected. They may represent doublets or multiplets (i.e. two or more cells in the same droplet, therefore share the same cell barcode)
3. Cells with high mitochondrial transcript percentage. As most of the scRNA-seq experiments use oligo-T to capture mRNAs, mitochondrial transcripts should be relatively under-representative due to their lack of poly-A tails, but it is avoidless that some mitochondrial transcripts are captured. Meanwhile, there are also evidences that stable poly-A tails exist in some mitochondrial transcripts but serve as a marker for degradation (e.g. this [paper](https://mcb.asm.org/content/25/15/6427.long)). Together, cells with high mitochondrial transcript percentage may represent cells under stress like hypoxia that produce lots of mitochondria, or produce abnormally high amount of truncated mitochondrial transcripts.

While numbers of detected genes are summarized by Seurat automatically when creating the Seurat object (with default feature name nFeature_RNA; nCount_RNA is the number of detected transcripts), one needs to calculate mitochondial transcript percentages manually. Still, Seurat has a way to make it easier
```R
seurat[["percent.mt"]] <- PercentageFeatureSet(seurat, pattern = "^MT[-\\.]")
```

Please note that there is no one-for-all filtering criteria, as the normal ranges of these metrics can vary dramatically from one experiment to another, depending on sample origins as well as reagants and sequencing depths. One suggestion here is to **ONLY FILTER OUT OUTLIER CELLS**, i.e. those **minority** of cells with certain QC metrics clearly exceeding or below other cells. To do that, one need to first know how the distribution of these values in the data. Make a violin plot for each of the metrics is what one can do.
```R
VlnPlot(seurat, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3)
```
<img src="images/vlnplot_QC.png" align="center" /><br/><br/>
Or if you don't like the dots
```R
VlnPlot(seurat, features = c("nFeature_RNA", "nCount_RNA", "percent.mt"), ncol = 3, pt.size=0)
```
<img src="images/vlnplot_QC_nopt.png" align="center" /><br/><br/>

And as one would expect, the numbers of detected genes and numbers of detected transcripts are well correlated across cells while mitochondrial transcript percentage is not.
```R
library(patchwork)
plot1 <- FeatureScatter(seurat, feature1 = "nCount_RNA", feature2 = "percent.mt")
plot2 <- FeatureScatter(seurat, feature1 = "nCount_RNA", feature2 = "nFeature_RNA")
plot1 + plot2
```
<span style="font-size:0.8em">*P.S. patchwork is an R package developped to facilitate layout of plots produced by ggplot2 (Seurat uses ggplot2 to produce plots if you use the plotting functions in the Seurat package). Without patchwork, it is illegal to run ```plot1 + plot2```.*</span>
<img src="images/scatterplot_QCmetrics.png" align="center" /><br/><br/>

Therefore, we only need to set cutoffs to either the detected gene number or detected transcript number, combining with an upper threshold of mitochondrial transcript percentage, for the QC. For instance, for this data set, detected gene numbers between 500 and 5000, and mitochondrial transcript percentage lower than 5% would be quite reasonable, but it is completely fine to use different thresholds.
```R
seurat <- subset(seurat, subset = nFeature_RNA > 500 & nFeature_RNA < 5000 & percent.mt < 5)
```

### Step 3. Normalization
Similar to bulk RNA-seq, the amount of captured RNA in each cell is different, therefore a direct comparison of the captured transcript number of each gene across different cells is definitely not the right way to do. A normalization step, aiming to make gene expression levels of different cells comparable, is therefore necessary. The most commonly used normalization in scRNA-seq data analysis is very similar to the concept of TPM (Transcripts Per Million reads), to normalizes the feature expression measurements for each cell by the total expression, and then multiplies this by a scale factor (10000 by default). At the end, the resulted expression levels are log-transformed so that the resulted values can better fit normal distribution. It is worth to mention, that before doing log-transform, one pseudocount is added to every value, so that genes with zero transcript detected in a cell still present present values of zero after log-transform.
```R
seurat <- NormalizeData(seurat)
```
In principle there are several parameters one can set in the ```NormalizeData``` function but in most of the time one just needs the default setting.

### Step 4. Feature selection for following heterogeneity analysis
The biggest advantage of doing scRNA-seq experiment is the potential to look into cell heterogeneity of samples, by looking for cell groups with distinct molecular signatures. However, not every gene has the same level of information and the same contribution when trying to identify different cell groups. For instance, genes with low expression levels, and those with similar expression levels across all cells, are not very informative and may introduce troubles by diluting differences among different cell groups. Therefore, doing a proper feature selection is very necessary when analyzing scRNA-seq data.

In Seurat, or more general in scRNA-seq data analysis, this step usually refers to identification of highly variable features/genes, which means to identify genes with the most varied expression levels in cells.
```R
seurat <- FindVariableFeatures(seurat, nfeatures = 3000)
```
By default, Seurat calculates standardized variance of each gene, and pick the top 2000 ones as the highly variable features. One can change the number of highly variable features easily by giving the ```nfeatures``` option (here the top 3000 genes are used).

There is no good criteria to determine how many highly variable features to use. Sometimes one needs to do some iteration to pick the number giving the most clear and interpretable result. In most of time, a value between 2000 to 5000 is OK and using a different value usually don't affect the results very much.

One can visualize the result via a variable feature plot but this is very optional.
```R
top_features <- head(VariableFeatures(seurat), 20)
plot1 <- VariableFeaturePlot(seurat)
plot2 <- LabelPoints(plot = plot1, points = top_features, repel = TRUE)
plot1 + plot2
```
<img src="images/variablefeatures.png" align="center" /><br/><br/>

### Step 5. Data scaling
Since different genes have different base expression levels and variations, their contributions to the analysis are different if no data transformation is done. This is not something we want as we don't want our analysis only represent the behaviors of highly expressed genes. Therefore, as what is usually done in any data science field, a scaling to the data is applied to the selected features.
```R
seurat <- ScaleData(seurat)
```

At this step, one can also remove unwanted source of variation from the data set by setting the parameter ```var.to.regress```. For instance,
```R
seurat <- ScaleData(seurat, vars.to.regress = c("nFeature_RNA", "percent.mt"))
```
Variables which are commonly considered to regress out include the number of detected genes/transcripts (nFeature_RNA / nCount_RNA), mitochondrial transcript percentage (percent.mt), and cell cycle related varaibles (will mention later). However, doing so dramatically slow down the whole process and the result is not necessarily improved. Therefore, a common suggestion is to first of all not to do any regress-out, check the result, and if the unwanted variation source dominated the heterogeneity, try to regress out the variable and see whether the result improves.

### (Optional and advanced) Alternative step 3-5: to use SCTransform
One problem of doing the typical log-normalization is [introducing the zero-inflation artifact](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-019-1874-1) to the scRNA-seq data. To better resolve this issue, Hafemeister and Satija introduced an R package ```sctransform```, which uses a regularized negative binomial regression model to normalize scRNA-seq data. Seurat has a wrapper function ```SCTransform```.
```R
seurat <- SCTransform(seurat, variable.features.n = 3000)
```
The ```variable.features.n``` controls the number of highly variable features to identify. One can also put in unwanted variation source to the model to try to remove them. For instance,
```R
seurat <- SCTransform(seurat, vars.to.regress = c("nFeature_RNA", "percent.mt"), variable.features.n = 3000)
```
This operation combines normalization, scaling and highly variable feature identification so it essentially replaces the above Step 3 to step 5. Drawbacks of running ```SCTransform``` include
1. It is slow.
2. It makes the normalized expression measurements data-dependent. In the typical procedure, the normalization only relies on the cell itself; in ```SCTransform```, however, information from the other cells in the same data set is involved during normalization. This potentially introduces problems when multiple data sets are needed to be compared, as technically speaking, the normalized expression measurements of two data sets normalized using ```SCTransform``` on their own are not comparable.
3. There are steps in ```SCTransform``` which involve random sampling to speed up the computation. It means there is stochastics in ```SCTransform``` so the result is different from one time to another, even if it is applied to the same data set.

Therefore, use it wisely.

### Step 6. Linear dimension reduction using principal component analysis (PCA)
In principle one can start to look at cell heterogeneity after identification of highly variable genes and scaling the data. However, applying a linear dimension reduction before doing any further analysis is strongly recommended and sometimes even seen as compulsory. The benefit of doing such a dimension reduction includes but not limits to:
1. The data becomes much more compact so that computation becomes much faster.
2. As scRNA-seq data is intrincically sparse, summarizing measurements of related features greatly enhances the signal robustness

Drawback? Basically nothing. Well, one needs some extra lines in the script and needs to decide the number of reduced dimension to use in the following analysis, but that's it. If so, what's the point not to do it?

For scRNA-seq data, the linear dimension reduction mostly refers to principal component analysis, or PCA.
```R
seurat <- RunPCA(seurat, npcs = 50)
```
In principle, the number of PCs in your data that one can calculate is the smaller value between the number of highly variable genes and the number of cells. However, most of these PCs are not informative and only represent random noise. Only the top PCs are informative and represent differences among cell groups. Therefore, instead of calculating all possible PCs, Seurat uses truncated PCA to only calculate the first PCs, by default the top 50 PCs. One can change that by setting the ```npcs``` parameter.

Even then, one doesn't necessarily use all the calculated PCs. Indeed, determinting how many top PCs to use is an art. There is no golden standard, and everyone has his/her own understanding. Usually, people use the elbowplot to assist making the decision.
```R
ElbowPlot(seurat, ndims = ncol(Embeddings(seurat, "pca")))
```
<span style="font-size:0.8em">*P.S. ```Embeddings``` is the function in Seurat to obtain the dimension reduction result given the name of the dimension reduction of interest. By default, the ```RunPCA``` function stores the PCA result in the embedding called 'pca', with each column being one PC. So here it tells Seurat to make the elbowplots to show the standarized variation of all the PCs that are calculated*</span>

<img src="images/elbowplot.png" align="center" /><br/><br/>

As it's defined, low-rank PCs have smaller standard deviations. However, such decrease of standard deviation is not linear. It drops dramatically at the very beginning, and then slow down and soon becomes pretty flat. One would then assume that the first phase of the curve represent the 'real' signal related to cell group differences, while the second phase represent mostly fluctuation of measurement or the stochastic nature of individual cells. To that perspective, choosing the top-15 PCs is probably good and PCs ranked lower than 20 look quite unnecessary. However, even though this is a pretty good reference, it is far from perfect:
  * It is very difficult to precisely define the elbow point or turning point of the curve, as it is usually not a perfect elbow.
  * Higher-ranked PCs do explain more variation than lower-ranked PCs, but in real world more explained variations don't necessaily mean more informative. Sometimes there are interesting but weak signals buried among noises and can only recovered in some lower-ranked PCs.

There is another procedure implemented in Seurat called ```JackStraw``` which can also serve as another reference. However, to our experience, it is very slow because it relies on data permutations and essentially it does not provide much more information than the elbow plot. It does estimates statistical significance of each PC but similarly, a 'significant' PC doesn't mean it is informative. And when cell number increases, more and more PCs become statistically 'significant' even though their explained variation is not substantial. People interested in this method can take a look at the Seurat [vignette](https://satijalab.org/seurat/v3.1/pbmc3k_tutorial.html).

Besides making the decision unbiasly, one can also check for each of the top PCs which genes are mostly contributing. This could be 
